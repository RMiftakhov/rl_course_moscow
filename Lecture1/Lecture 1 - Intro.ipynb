{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning course at Moscow School of AI\n",
    "\n",
    "<img src=\"IntroImages/Event Cover.png\" alt=\"Drawing\" style=\"width: 1200px;\" />\n",
    "\n",
    "The course of lectures is composed so that you can understand the basics of RL as MDP and Markov Games, train an intelligent agent using the algorithms like DQN, PPO, A3C, MADDPG and ultimately apply the knowledge to solve problems that you choose for yourself.\n",
    "\n",
    "## Application of RL\n",
    "\n",
    "Reinforcement learning is about building code that can learn complex tasks by itself. Deep Reinforcement Learning, where the approach is represented by a deep neural network.  The computer is able to win the world champion on the game of \"Go\" without knowing any rule of the game and based on the winning or losing it improve its strategy. Reinforcement learning helps to AlphaZero to learn from the experience so it can gradually enhance the decision-making process and eventually winning the game.  RL is not limited only to board games, it can be used in many fields for example: teach the robot to walk and manipulate with objects, self-driving cars, stock market prediction, etc. RL is a step towards General Artificial Intelligence, but we quite far from it. \n",
    "\n",
    "* Autonomous driving\n",
    "* Flying cars\n",
    "* Stock market prediction\n",
    "* Generally any decision-making kind of job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Car](IntroImages/intro.jpg)\n",
    "\n",
    "\n",
    "<img src=\"IntroImages/soccer.gif\" alt=\"Drawing\" style=\"width: 900px;\" />\n",
    "<img src=\"IntroImages/agent_example.PNG\" alt=\"Drawing\" style=\"width: 900px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chess - MCTS\n",
    "**Elo rating** - is a number that measures a relative skill level of a player. \n",
    "\n",
    "<img src=\"IntroImages/top_chess_players.PNG\" alt=\"Drawing\" style=\"width: 900px;\" />\n",
    "\n",
    "<img src=\"IntroImages/carlsen_magnus.png\" alt=\"Drawing\" style=\"width: 900px;\" />\n",
    "\n",
    "* | *\n",
    "- | -\n",
    "<img src=\"IntroImages/stockfish_1.PNG\" alt=\"Drawing\" style=\"height: 400px;\" /> | <img src=\"IntroImages/stockfish_3.PNG\" alt=\"Drawing\" style=\"height: 400px;\" />\n",
    "\n",
    "<img src=\"IntroImages/alpha_zero.PNG\" alt=\"Drawing\" style=\"width: 700px;\" />\n",
    "\n",
    "### Dota 2 - MARL\n",
    "\n",
    "<img src=\"IntroImages/dota2.gif\" alt=\"Drawing\" style=\"width: 900px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL framework\n",
    "\n",
    "\n",
    "In general, the RL setting consists of an agent and an environment. \n",
    "At initial time-step, an agent observes the environment. Then an agent must select an appropriate action in response.  An environment in response to action presents a new observation and a reward, which has an evaluation of the agent's taken action.\n",
    "\n",
    "* | *\n",
    "- | -\n",
    "<img src=\"IntroImages/agent_env_berkeley.PNG\" alt=\"Drawing\" style=\"height: 400px;\" /> | <img src=\"IntroImages/agent_env.png\" alt=\"Drawing\" style=\"height: 400px;\" />\n",
    "\n",
    "The goal of the Agent: Maximize expected cumulative reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Environment\n",
    "\n",
    "* Gym Environment\n",
    "* Unity ML\n",
    "\n",
    "<img src=\"IntroImages/envs.gif\" alt=\"Drawing\" style=\"width: 900px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pong from pixels  (PyTorch)\n",
    "* Compute device selection\n",
    "* Gym environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom utilies for displaying animation, collecting rollouts and more\n",
    "import pong_utils\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check which device is being used. \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device: \",device)\n",
    "# render ai gym environment\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# PongDeterministic does not contain random frameskip\n",
    "# so is faster to train than the vanilla Pong-v4 environment\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n",
    "\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life\n",
    "# the actions are hard-coded in pong_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "* Downsampling to 80 by 80\n",
    "* Removing background\n",
    "\n",
    "On the low level, the game works as follows: we receive an image frame (a 210x160x3 byte array (integers from 0 to 255 giving pixel values)), and we get to decide if we want to move the paddle UP or DOWN (i.e., a binary choice). After every single choice, the game simulator executes the action and gives us a reward: Either a +1 reward if the ball went past the opponent, a -1 reward if we missed the ball, or 0 otherwise. And of course, our goal is to move the paddle so that we get lots of rewards. [Andrej Karpathy blog](http://karpathy.github.io/2016/05/31/rl/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show what a preprocessed image looks like\n",
    "env.reset()\n",
    "_, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(pong_utils.preprocess_single(frame), cmap='Greys')\n",
    "plt.show()\n",
    "print (\"Original shape is: {}\".format(frame.shape))\n",
    "print (\"Preprocess shape is: {}\".format(pong_utils.preprocess_single(frame).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLICY\n",
    "\n",
    "This network will take the state of the game and decide what we should do (move UP or DOWN). As our favorite simple block of a computer, weâ€™ll use a 2-layer neural network that takes the raw image pixels \n",
    "\n",
    "<img src=\"IntroImages/state_to_action_example.PNG\" alt=\"Drawing\" style=\"width: 800px;\" />\n",
    " \n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$.\n",
    "We use the sigmoid non-linearity at the end, which squashes the output probability to the range [0,1]. \n",
    "\n",
    "**Credit assignment problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # 80x80x2 to 38x38x4\n",
    "        # 2 channel from the stacked frame\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size=6, stride=2, bias=False)\n",
    "        # 38x38x4 to 9x9x32\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=6, stride=4)\n",
    "        self.size=9*9*16\n",
    "        \n",
    "        # two fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        # Sigmoid to \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1,self.size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.sig(self.fc2(x))\n",
    "\n",
    "policy=Policy().to(device)\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL connection to Supervised Learning\n",
    "\n",
    "<img src=\"IntroImages/supervised_learning_example.PNG\" alt=\"Drawing\" style=\"width: 800px;\" />\n",
    "\n",
    "If the agent learning is represented as a Supervised Learning problem, then the agent will never be better than those data that it was trained on. If we talk about RL learning, then we assume that we can achieve **super-human** level of performance. \n",
    "\n",
    "RL Policy Gradients is very similar to Supervised Learning.\n",
    "\n",
    "<img src=\"IntroImages/connection_to_sl.png\" alt=\"Drawing\" style=\"width: 800px;\" />\n",
    "\n",
    "Supervised learning setup, we have a bunch of labeled data that is feed into NN, in this context learning means tweaking the weights (back-propagation) of the NN to identify a given picture correctly. Changing the weights increases the probability of providing the right label. In RL setup, we collect many episodes by following some policy that is labeled at the end of the episode by winning or losing the game, and then actions are the same as pictures in Supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients\n",
    "\n",
    "* Sample an action from this distribution; E.g., suppose we sample DOWN, and we will execute it in the game. \n",
    "* Wait until the end of the game, then take the reward we get (either +1 if we won or -1 if we lost), and enter that scalar as the gradient for the action we have taken (DOWN in this case).\n",
    "* Fill in -1 for log probability of DOWN and do backprop we will find a gradient that discourages the network to take the DOWN action for that input in the future.\n",
    "[Andrej Karpathy blog](http://karpathy.github.io/2016/05/31/rl/)\n",
    "\n",
    "<img src=\"IntroImages/forward_pass.PNG\" alt=\"Drawing\" style=\"width: 800px;\" />\n",
    "\n",
    "### Reflection\n",
    "\n",
    "**Policy Gradients**: Run a policy for a while. See what actions led to high rewards. Increase their probability.\n",
    "\n",
    "<img src=\"IntroImages/agent_learning.PNG\" alt=\"Drawing\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "<img src=\"IntroImages/score_function.PNG\" alt=\"Drawing\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE algorithm\n",
    "\n",
    "REINFORCE is the algorithm that can be used to find the best weights for a policy network that maximizes the expected return U. \n",
    "\n",
    "1. Use the policy $\\pi_{\\theta}$ to collect m trajectories $\\tau^{1}, \\tau^{2}, ..., \\tau^{m}$ with horizont $H$. We refer to the $i$-th trajectory as\n",
    "$$\\tau^{i} = (s_0^{i}, a_0^{i}, ..., s_H^{i}, a_H^{i}, s_{H+1}^{i})$$\n",
    "2. In the REINFORCE algorithm log of probability it used to increase/decrease the occurrence of the action in the trajectory. Use the trajectories to estimate the gradient $\\nabla_{\\theta}U(\\theta)$:\n",
    "\n",
    "$$\\nabla_{\\theta}U(\\theta) \\approx \\hat{g} = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{t=0}^{H} \\nabla_\\theta log \\pi_\\theta (a_t^{i}|s_t^i) R(\\tau^i)$$\n",
    "3. Update the weights of the policy: \n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\hat{g}$$\n",
    "4. Loop over steps 1-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE\n",
    "you have two choices (usually it's useful to divide by the time since we've normalized our rewards and the time of each trajectory is fixed)\n",
    "\n",
    "1. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\log(\\pi_{\\theta'}(a_t|s_t))$\n",
    "2. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}$ where $\\theta'=\\theta$ and make sure that the no_grad is enabled when performing the division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "RIGHT=4\n",
    "LEFT=5\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# convert states to probability, passing through the policy\n",
    "def states_to_prob(policy, states):\n",
    "    states = torch.stack(states)\n",
    "    policy_input = states.view(-1,*states.shape[-3:])\n",
    "    return policy(policy_input).view(states.shape[:-3])\n",
    "# return sum of log-prob divided by T\n",
    "# same thing as -policy_loss\n",
    "def surrogate(policy, old_probs, states, actions, rewards,\n",
    "              discount = 0.995, beta=0.01):\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    return torch.mean(ratio*rewards + beta*entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING\n",
    "We are now ready to train our policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 800\n",
    "\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "# initialize environment\n",
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "beta = .01\n",
    "tmax = 100\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        pong_utils.collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "  \n",
    "    L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    optimizer.zero_grad()\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "    del L\n",
    "        \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play game after training!\n",
    "pong_utils.play(env, policy, time=2000) \n",
    "torch.save(policy, 'REINFORCE.policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whatch a smart agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "policy_solution = torch.load('REINFORCE_solution.policy', map_location='cpu')\n",
    "pong_utils.play(env, policy_solution, time=2000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content of this course \n",
    "The content is divided into four parts.\n",
    "\n",
    "1. Intro to Deep Reinforcement Learning\n",
    "     * The RL Framework: MDP\n",
    "     * Monte-Carlo Methods \n",
    "     * Temporal-Difference Methods (SARSA and Q-learning)\n",
    "     * Project 1\n",
    "1. Value-Based Methods\n",
    "    * Deep Q-Networks\n",
    "    * Project 2\n",
    "1. Policy-Based Methods\n",
    "    * Intro to Policy-Based Methods\n",
    "    * Black-Box optimization\n",
    "    * REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility (REINFORCE)\n",
    "    * Proximal Policy Optimization (PPO)\n",
    "    * Actor-Critic Methods (A3C, DDPG)\n",
    "    * Project 3\n",
    "1. Multi-Agent Reinforcement Learning\n",
    "    * Intro to Multi-Agent RL\n",
    "    * AlphaZero\n",
    "    * Project 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* | *\n",
    "- | -\n",
    "<img src=\"IntroImages/LunarLander.gif\" alt=\"Drawing\" style=\"height: 300px;\" /> | <img src=\"IntroImages/Banana.gif\" alt=\"Drawing\" style=\"height: 300px;\" />\n",
    "<img src=\"IntroImages/Reacher.gif\" alt=\"Drawing\" style=\"height: 300px;\" /> | <img src=\"IntroImages/Tennis.gif\" alt=\"Drawing\" style=\"height: 300px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IntroImages/multi_agent_rl.png\" alt=\"Drawing\" style=\"height: 500px;\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
