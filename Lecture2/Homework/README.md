# Deep Reinforcement Learning course at Moscow School of AI: Homework 1

The homework is based on the [materials](https://github.com/udacity/deep-reinforcement-learning/tree/master/lab-taxi) from [Deep Reinforcement Learning Nanodegree (Udacity)](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).

RL - Reinforcement Learning - Обучение с подкреплением;

## Такси
Репозиторий содержит три файла:

* agent.py: Разработайте архитектуру своего RL агента. Это единственный файл, который вы должны изменить;
* monitor.py: Функция  **interact** определяет насколько хорошо ваш агент учится на основе взаимодействия с окружением;
* main.py: Запустите этот файл в терминале, чтобы проверить производительность вашего агента.

Начните исполнение со следующей команды в терминале:

```python
python main.py
```

Когда вы запускаете `main.py`, агент, указанный вами в `agent.py`, взаимодействует со средой в течение 20 000 эпизодов. Детали взаимодействия указаны в файле `monitor.py`, который возвращает две переменные: `avg_rewards` и `best_avg_reward`.

* `avg_rewards` - это deque, где `avg_rewards [i]` - это средняя (недисконтированная) награда, полученная агентом от эпизодов `i + 1` до `i + 100`, включительно. Так, например, `avg_rewards [0]` - это средняя награда, полученная агентом за первые 100 эпизодов.
* `best_avg_reward` - самая большая награда в avg_rewards. Это окончательная оценка, которую вы должны использовать при определении того, насколько хорошо ваш агент выполнил задание.

Ваше задание состоит в том, чтобы изменить файл `agents.py` для повышения производительности агента.

* Используйте метод `__init __ ()` для определения любых необходимых переменных. В настоящее время мы определяем количество действий, доступных для агента (nA), и инициализируем пустым словарем ценность действий (action values) (Q).
* Метод `select_action ()` принимает состояние среды в качестве входных данных и возвращает выбор действия агента. Код в репозитории случайным образом выбирает действие.
* Метод `step ()` принимает (state, action, reward, next_state) в качестве входных данных вместе с переменной done, которая равна True, если эпизод закончился. Вы должны изменить этот метод, чтобы использовать накопленные знания об окружении для обучения агента.

После того, как вы изменили функцию, вам нужно запустить `python main.py` для проверки вашего нового агента.

OpenAI Gym [определяет «решение»](https://gym.openai.com/envs/Taxi-v1/) этой задачи как получение средней награды 9,7 за 100 последовательных запусков.